# Аналіз тональності тексту за допомогою нейромереж

<br>Уявіть собі, що у вас є якась частина тексту, який ви ще не прочитали, але ви хочете знати, який він містить в собі настрій: радість, сум чи гнів?
В даному випадку ми будем класифікувати нашу задачу на 2 основні типи емоцій: ***позитині*** та ***негативні***.</br>
<br>Є багато способів вирішувати таке завдання. Один з них - це **згорткові нейронні мережі** (Convolutional Neural Networks). 
Ці нейронні мережі спочатку були розроблені для обробки зображень, однак вони успішно справляються з вирішенням завдань у сфері автоматичної обробки текстів. 
В даній роботі ми розглянемо бінарний аналіз тональності текстів українською за допомогою згорткової нейронної мережі,
для якої векторні представлення слів були сформовані на основі навченої **Word2Vec** моделі.</br>
![Sentiment_analysis_main](https://github.com/t1pUr/software_engineering_template/blob/master/src/images/Sentiment%20analysis%20main.gif)
<br>Джерело: https://habrastorage.org/webt/2u/l3/lw/2ul3lwsbyobovjnol2g_cbvrghi.gif</br>
<h2>
Застосування згорткових нейронних мереж (Convolutional Neural Networks) для задач NLP
</h2>
<br>
З самого початку згорткові нейронні мережі були призначені для розпізнавання та обробки зоображень. Через те, що вони влаштовані на зразок зорової кори головного мозку - тобто вміють концентруватися на невеликій області і виділяти в ній важливі особливості, CNN досягли значного успіху не тільки при роботі з зображеннями, але і для задач нейролінгвістичного програмування (Natural Language Processing, NLP).
</br>

<br>Що ж таке згортка? Розберемо приклад, а саме візуалізацію від Стенфорда:</br>
![Sentiment_analysis_main](https://github.com/t1pUr/software_engineering_template/blob/master/src/images/CNN_example.gif)
<br>Джерело: https://habrastorage.org/getpro/habr/post_images/642/8cf/505/6428cf505ac1e9e1cf462e1ec8fe9a68.gif</br>
<br>Вікно, яке ходить по великій матриці називається фільтром (в англомовному варіанті ***kernel***, ***filter*** або ***feature detector***, можна зустріти декілька варіантів цих термінів). Фільтр накладається на ділянку великої матриці і кожне значення перемножується з відповідним йому значенням фільтра (червоні цифри нижче і правіше чорних цифр основної матриці). Потім все що вийшло складається і виходить кінцеве ("відфільтроване") значення. Цих фільтрів існує безліч та вони можуть ходити декілька разів по одній матриці, оскільки один й той самий фільтр може звертати увагу на різні деталі.</br>
<br>Вікно ходить по великій матриці з якимось кроком, який по-англійськи називається ***stride***. Цей крок буває горизонтальний і вертикальний (хоча останній нам не знадобиться).</br>
<br>Ще один важливий концепт - **канал**. Каналами в зображеннях називаються відомі багатьом базові кольори, наприклад, якщо ми говоримо про просту і поширену схему колірного кодування RGB (Red - червоний, Green - зелений, Blue - блакитний), то там передбачається, що з цих трьох базових кольорів, шляхом їх змішування ми можемо отримати будь-який колір.</br>
![Sentiment_analysis_main](https://github.com/t1pUr/software_engineering_template/blob/master/src/images/channels.png)
<br>Джерело: https://neurohive.io/wp-content/uploads/2018/07/kernels.png</br>

<br>Отже, у нас є зображення, в ньому є канали і по ньому з потрібним кроком ходить наш фільтр, але що саме відбувається з каналами? З цими каналами ми робимо наступне - кожен фільтр (тобто матриця невеликого розміру) накладається на вихідну матрицю одночасно на всі три канали. Результати ж просто підсумовуються.</br>

<br>Також є ще один базовий шар у CNN. Це так званий ***pooling-шар***. Розглянемо цей шар на прикладі ***max-pooling***. Уявіть, що у вже відомому вам згортковому шарі матриця фільтра зафіксована і є одиничною (тобто множення на неї ніяк не впливає на вхідні дані). А замість підсумовування всіх результатів множення ми просто вибираємо максимальний елемент. Тобто ми виберемо з усього вікна піксель з найбільшою інтенсивністю. Це і є max-pooling. Звичайно, замість цієї функцій може бути й інша арифметична, або навіть складніша функція.</br>
![Sentiment_analysis_main](https://github.com/t1pUr/software_engineering_template/blob/master/src/images/max_pooling.png)
<br>Джерело: https://habrastorage.org/getpro/habr/post_images/e38/ee6/f95/e38ee6f954b7c7cea8f768c77eaff301.png</br>
<br>Тепер трохи відступимо від цієї теми та перейдемо до ***word embededing***</br>
<h2>
Word embedding або векторне представлення слів
</h2>

<br>Звідки береться сама задача word embedding?
На жаль, поки що не існує єдиного терміна для цього поняття, тому ми будемо використовувати англомовний.
Сам по собі embedding - це зіставлення довільної сутності (наприклад, вузла в графі або шматочка картинки) деякого вектору.</br>
![Sentiment_analysis_main](https://github.com/t1pUr/software_engineering_template/blob/master/src/images/embedding.png)
<br>Джерело: https://habrastorage.org/getpro/habr/post_images/3e8/12f/d16/3e812fd164a08f5e4f195000fecf988f.png</br> 

<br>Ось у нас є слова і є комп'ютер, який повинен з цими словами якось працювати. Питання - як комп'ютер буде працювати зі словами? Адже комп'ютер не вміє "читати". Перше, що приходить в голову - просто закодувати слова цифрами по порядку проходження в словнику. Ідея дуже продуктивна в своїй простоті - натуральний ряд нескінченний і можна пронумерувати всі слова, не боючись проблем.</br>

<br>Але у цієї ідеї є і істотний недолік: слова в словнику слідують в алфавітному порядку, і при додаванні слова потрібно перенумеровувати заново більшу частину слів. Але навіть це не є настільки важливим, а важливо те, що буквене написання слова ніяк не пов'язане з його змістом. Наприклад, слова "півень", "курка" і "курча" мають дуже мало спільного між собою і стоять в словнику далеко один від одного, хоча очевидно позначають самця, самку і дитинча одного виду птиці. Тобто ми можемо виділити два види близькості слів: лексичний і семантичний. Як ми бачимо на прикладі з куркою, ці близькості не обов'язково збігаються. Можна для наочності привести зворотний приклад лексично близьких, але семантично далеких слів - "зола" та "золото". Щоб отримати можливість представити семантичну близькість, було запропоновано використовувати **embedding**, тобто зіставити слова в якийсь вектор, що відображає його значення в "просторі смислів".</br>

<br>Найпростішим способом буде взяти вектор довжини нашого словника і поставити тільки одиницю в позиції, що відповідає номеру слова в словнику. Цей підхід називається **one-hot encoding (OHE)**. Але **OHE** все ще не має властивості семантичної близькості:</br>
![Sentiment_analysis_main](https://github.com/t1pUr/software_engineering_template/blob/master/src/images/one_hot_encoding.png)
<br>Джерело: http://neerc.ifmo.ru/wiki/images/4/49/One-hot-encoding.png</br> 

